{"cells":[{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1747740053188,"user":{"displayName":"Anwesh Nayak","userId":"10812060260948452341"},"user_tz":-330},"id":"T2afuhmpwsic"},"outputs":[],"source":["from transformers import ViltProcessor, ViltForQuestionAnswering\n","import torch\n","from PIL import Image\n","import requests\n","import json\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import os\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","#from word2number import w2n"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1911,"status":"ok","timestamp":1747740055102,"user":{"displayName":"Anwesh Nayak","userId":"10812060260948452341"},"user_tz":-330},"id":"kEU28nTOxQoE","outputId":"91179d6f-6566-4c18-d4c6-f68bb9bec4f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1747740055135,"user":{"displayName":"Anwesh Nayak","userId":"10812060260948452341"},"user_tz":-330},"id":"gR3eeKXmUfE3"},"outputs":[],"source":["# Path to your merged VQA JSON (with filename-only paths)\n","json_path = '/content/drive/MyDrive/VR Project 2/Curated Dataset/final_VQA_dict_merged_1234_2.json'\n","\n","# Load into a Python dict: {image_id: record}\n","with open(json_path, 'r', encoding='utf-8') as f:\n","    vqa_data = json.load(f)"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1747740055167,"user":{"displayName":"Anwesh Nayak","userId":"10812060260948452341"},"user_tz":-330},"id":"eCXT4t9IUm6d","outputId":"2172a471-e688-48ed-fa03-ab4c0210109f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'What is this product?': {'correct_option': 'Wall Clock',\n","  'plausible_options': ['Desk Clock', 'Alarm Clock', 'Clock Radio']},\n"," 'Which category best describes this product?': {'correct_option': 'Wall Clocks',\n","  'plausible_options': ['Clocks', 'Home Decor', 'Kitchen Decor']},\n"," 'What is the primary color of this Wall Clock?': {'correct_option': 'Black',\n","  'plausible_options': ['Brown', 'White', 'Gold']},\n"," 'What year is displayed on the clock face?': {'correct_option': '1965',\n","  'plausible_options': ['1975', '1985', '2000']},\n"," 'What is the shape of the clock?': {'correct_option': 'Round',\n","  'plausible_options': ['Square', 'Rectangular', 'Oval']},\n"," 'Is the clock made of wood?': {'correct_option': 'yes',\n","  'plausible_options': ['no', 'partially', 'not visible']},\n"," 'path': '/content/drive/MyDrive/VR Project 2/Data/abo-images-small/images/small/cc/cc9f405c.jpg'}"]},"metadata":{},"execution_count":34}],"source":["vqa_data['91wdUx01uXL']"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1747740127191,"user":{"displayName":"Anwesh Nayak","userId":"10812060260948452341"},"user_tz":-330},"id":"Z2nfUVw3TxBw"},"outputs":[],"source":["examples = []\n","for image_id, rec in vqa_data.items():\n","    img_path = os.path.join(\n","        '/content/drive/MyDrive/VR Project 2/Data Curation/Selected Images',\n","        rec['path']\n","    )\n","    # print(rec)\n","    # break\n","    # For each question in the record (skip 'path')\n","    for question, qa in rec.items():\n","        # print(qa)\n","        if question == 'path':\n","            continue\n","        # question = qa['question']\n","        if 'correct_option' in qa.keys():\n","            correct = qa['correct_option']\n","        else:\n","            correct = qa['correct']\n","        # if 'plausible_options' in qa.keys():\n","        #     options = qa['plausible_options'][:]\n","        # elif 'options' in qa.keys():\n","        #     options = qa['options'][:]\n","        # else:\n","        #     options = qa['plausible'][:]\n","        # ensure correct is in options\n","        # if correct not in options:\n","        #     # print('*******')\n","        #     options.insert(0, correct)\n","        # label = options.index(correct)\n","        examples.append({\n","            'Image_ID': image_id,\n","            'Question': question,\n","            'Image_Path': rec['path'],\n","            'Answer': correct\n","        })\n","\n","#Image_ID,Item_ID,Question,Answer,Image_Path"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":135,"status":"ok","timestamp":1747740130168,"user":{"displayName":"Anwesh Nayak","userId":"10812060260948452341"},"user_tz":-330},"id":"5v8IXSrowLe0","outputId":"3c0df786-bb20-497e-e452-4ae0af9f066f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["          Image_ID                                        Question  \\\n","0      71ovuTiG8AL         What is the primary color of this Desk?   \n","1      71P7CM5zw-L   What is the primary color of this TV Trolley?   \n","2      71IjzZFmmiL           What type of earring setting is used?   \n","3      21t4oVa+FRL            What is the shape of the containers?   \n","4      810jM9UbU7L       Is the fitted sheet shown made of cotton?   \n","...            ...                                             ...   \n","15509  51Ztzh1HAHL     Which category best describes this product?   \n","15510  81MXFSxSGbL                           What is this product?   \n","15511  81CI0nfUdVL  How many cubes are there in this storage unit?   \n","15512  91l2nw9egVL     Which category best describes this product?   \n","15513  41UpW1lleJL      What is the primary color of this cup lid?   \n","\n","                                              Image_Path                Answer  \n","0      /content/drive/MyDrive/VR Project 2/Data/abo-i...           Light Brown  \n","1      /content/drive/MyDrive/VR Project 2/Data/abo-i...                 Black  \n","2      /content/drive/MyDrive/VR Project 2/Data/abo-i...                  Stud  \n","3      /content/drive/MyDrive/VR Project 2/Data/abo-i...                Square  \n","4      /content/drive/MyDrive/VR Project 2/Data/abo-i...                   Yes  \n","...                                                  ...                   ...  \n","15509  /content/drive/MyDrive/VR Project 2/Data/abo-i...  Disposable Tableware  \n","15510  /content/drive/MyDrive/VR Project 2/Data/abo-i...              TV Mount  \n","15511  /content/drive/MyDrive/VR Project 2/Data/abo-i...                  Four  \n","15512  /content/drive/MyDrive/VR Project 2/Data/abo-i...               Storage  \n","15513  /content/drive/MyDrive/VR Project 2/Data/abo-i...                 Clear  \n","\n","[15514 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-404653b0-3e5a-4bf1-9053-e2d9128a225e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Image_ID</th>\n","      <th>Question</th>\n","      <th>Image_Path</th>\n","      <th>Answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>71ovuTiG8AL</td>\n","      <td>What is the primary color of this Desk?</td>\n","      <td>/content/drive/MyDrive/VR Project 2/Data/abo-i...</td>\n","      <td>Light Brown</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>71P7CM5zw-L</td>\n","      <td>What is the primary color of this TV Trolley?</td>\n","      <td>/content/drive/MyDrive/VR Project 2/Data/abo-i...</td>\n","      <td>Black</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>71IjzZFmmiL</td>\n","      <td>What type of earring setting is used?</td>\n","      <td>/content/drive/MyDrive/VR Project 2/Data/abo-i...</td>\n","      <td>Stud</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>21t4oVa+FRL</td>\n","      <td>What is the shape of the containers?</td>\n","      <td>/content/drive/MyDrive/VR Project 2/Data/abo-i...</td>\n","      <td>Square</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>810jM9UbU7L</td>\n","      <td>Is the fitted sheet shown made of cotton?</td>\n","      <td>/content/drive/MyDrive/VR Project 2/Data/abo-i...</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>15509</th>\n","      <td>51Ztzh1HAHL</td>\n","      <td>Which category best describes this product?</td>\n","      <td>/content/drive/MyDrive/VR Project 2/Data/abo-i...</td>\n","      <td>Disposable Tableware</td>\n","    </tr>\n","    <tr>\n","      <th>15510</th>\n","      <td>81MXFSxSGbL</td>\n","      <td>What is this product?</td>\n","      <td>/content/drive/MyDrive/VR Project 2/Data/abo-i...</td>\n","      <td>TV Mount</td>\n","    </tr>\n","    <tr>\n","      <th>15511</th>\n","      <td>81CI0nfUdVL</td>\n","      <td>How many cubes are there in this storage unit?</td>\n","      <td>/content/drive/MyDrive/VR Project 2/Data/abo-i...</td>\n","      <td>Four</td>\n","    </tr>\n","    <tr>\n","      <th>15512</th>\n","      <td>91l2nw9egVL</td>\n","      <td>Which category best describes this product?</td>\n","      <td>/content/drive/MyDrive/VR Project 2/Data/abo-i...</td>\n","      <td>Storage</td>\n","    </tr>\n","    <tr>\n","      <th>15513</th>\n","      <td>41UpW1lleJL</td>\n","      <td>What is the primary color of this cup lid?</td>\n","      <td>/content/drive/MyDrive/VR Project 2/Data/abo-i...</td>\n","      <td>Clear</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>15514 rows × 4 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-404653b0-3e5a-4bf1-9053-e2d9128a225e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-404653b0-3e5a-4bf1-9053-e2d9128a225e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-404653b0-3e5a-4bf1-9053-e2d9128a225e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-074be873-05d2-45f6-82f2-806c42c5e537\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-074be873-05d2-45f6-82f2-806c42c5e537')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-074be873-05d2-45f6-82f2-806c42c5e537 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_422650b0-4be0-436a-9d52-3cf18383c347\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_422650b0-4be0-436a-9d52-3cf18383c347 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 15514,\n  \"fields\": [\n    {\n      \"column\": \"Image_ID\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4352,\n        \"samples\": [\n          \"91YZNGe6gLL\",\n          \"71CDvaR9YKL\",\n          \"51b8ptJoz9L\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Question\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6158,\n        \"samples\": [\n          \"Is the cable braided?\",\n          \"Is the bookcase made of wood?\",\n          \"Is the USB connector reversible?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Image_Path\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4352,\n        \"samples\": [\n          \"/content/drive/MyDrive/VR Project 2/Data/abo-images-small/images/small/49/491d72cd.jpg\",\n          \"/content/drive/MyDrive/VR Project 2/Data/abo-images-small/images/small/78/78fbe9c1.jpg\",\n          \"/content/drive/MyDrive/VR Project 2/Data/abo-images-small/images/small/81/81c94d4b.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2407,\n        \"samples\": [\n          \"Gaming Chair\",\n          \"Moto G4 Plus\",\n          \"1/5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":37}],"source":["df = pd.DataFrame(examples)\n","df = df.sample(frac=0.6, random_state=42).reset_index(drop=True)\n","df"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1747740130415,"user":{"displayName":"Anwesh Nayak","userId":"10812060260948452341"},"user_tz":-330},"id":"GTjCYL7jz561","outputId":"f6c2833e-a2a7-4c34-9b98-efa8b51fbcdb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/VR Project 2/Data/abo-images-small/images/small/7c/7c944a0c.jpg'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":38}],"source":["df['Image_Path'][42]"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1747740130966,"user":{"displayName":"Anwesh Nayak","userId":"10812060260948452341"},"user_tz":-330},"id":"MpILRbtNwsif","outputId":"35a48b11-99fb-4506-f1b8-88c8162462f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train size: 10807, Val size: 2359, Test size: 2348\n"]}],"source":["# Load the CSV file\n","# csv_path = \"/kaggle/input/qna-final/qna_final.csv\"\n","# df = pd.read_csv(csv_path)\n","\n","# Get unique Item_IDs\n","unique_ids = df[\"Image_ID\"].unique()\n","\n","# Set random seed for reproducibility\n","random_seed = 42\n","np.random.seed(random_seed)\n","\n","# Shuffle and split the unique IDs\n","train_ids, temp_ids = train_test_split(unique_ids, test_size=0.3, random_state=random_seed)  # 70% train\n","val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=random_seed)      # 15% val, 15% test\n","\n","# Create train, val, and test DataFrames\n","train_df = df[df[\"Image_ID\"].isin(train_ids)]\n","val_df = df[df[\"Image_ID\"].isin(val_ids)]\n","test_df = df[df[\"Image_ID\"].isin(test_ids)]\n","\n","print(f\"Train size: {len(train_df)}, Val size: {len(val_df)}, Test size: {len(test_df)}\")\n"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28685,"status":"ok","timestamp":1747740160386,"user":{"displayName":"Anwesh Nayak","userId":"10812060260948452341"},"user_tz":-330},"id":"1r-a7OOkwsig","outputId":"e4946aec-df7f-4d7f-ee99-c4add27c008a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading sentence transformer model for semantic matching...\n","Computing embeddings for original vocabulary...\n","Creating semantic mappings for unseen answers...\n","Dataset train: Mapped 1477 unseen answers to semantically similar existing answers\n","Dataset val: Mapped 173 unseen answers to semantically similar existing answers\n","Dataset test: Mapped 169 unseen answers to semantically similar existing answers\n","\n","Answer mapping examples (with similarity scores):\n","  'none' -> 'None' (similarity: 1.000)\n","  'spoon and fork' -> 'fork and spoon' (similarity: 0.994)\n","  'yellow and pink' -> 'pink and yellow' (similarity: 0.993)\n","  'one inch' -> '1 inch' (similarity: 0.974)\n","  'grey' -> 'gray' (similarity: 0.968)\n","  'flip-flops' -> 'flip flops' (similarity: 0.966)\n","  'ball catch' -> 'catch ball' (similarity: 0.965)\n","  '1-inch' -> '1 inch' (similarity: 0.962)\n","  'aluminium' -> 'aluminum' (similarity: 0.960)\n","  'recliners' -> 'recliner' (similarity: 0.952)\n","  ... and 1809 more mappings\n","\n","Final sizes:\n","Train: 10807\n","Val: 2359\n","Test: 2348\n","Original vocabulary size: 3129\n","Total answer mappings created: 1819\n"]}],"source":["\"\"\"Normalizing and mapping non-existing answers to semantically similar existing answers in label2id\"\"\"\n","# Load the model and processor\n","model_config_source = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n","processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n","\n","# Text-to-number mapping (same as before)\n","def get_text_to_num_mapping():\n","    text_to_num = {\n","        \"zero\": \"0\", \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\",\n","        \"five\": \"5\", \"six\": \"6\", \"seven\": \"7\", \"eight\": \"8\", \"nine\": \"9\",\n","        \"ten\": \"10\", \"eleven\": \"11\", \"twelve\": \"12\", \"thirteen\": \"13\",\n","        \"fourteen\": \"14\", \"fifteen\": \"15\", \"sixteen\": \"16\", \"seventeen\": \"17\",\n","        \"eighteen\": \"18\", \"nineteen\": \"19\", \"twenty\": \"20\",\n","    }\n","    for i in range(21, 1001):\n","        text_to_num[str(i)] = str(i)\n","    return text_to_num\n","\n","text_to_num_map = get_text_to_num_mapping()\n","\n","def normalize_answer(answer_str):\n","    normalized = str(answer_str).strip().lower()\n","    return text_to_num_map.get(normalized, normalized)\n","\n","# Create DataFrames with .copy()\n","train_df = df[df[\"Image_ID\"].isin(train_ids)].copy()\n","val_df = df[df[\"Image_ID\"].isin(val_ids)].copy()\n","test_df = df[df[\"Image_ID\"].isin(test_ids)].copy()\n","\n","# Add normalized answers\n","train_df['normalized_answer'] = train_df['Answer'].apply(normalize_answer)\n","val_df['normalized_answer'] = val_df['Answer'].apply(normalize_answer)\n","test_df['normalized_answer'] = test_df['Answer'].apply(normalize_answer)\n","\n","# Get the original label2id\n","original_label2id = model_config_source.config.label2id\n","original_answers = list(original_label2id.keys())\n","\n","# Load sentence-transformers for semantic similarity\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","# Load a pre-trained model for semantic embeddings\n","print(\"Loading sentence transformer model for semantic matching...\")\n","semantic_model = SentenceTransformer('all-MiniLM-L6-v2')  # A lightweight model that works well for semantic similarity\n","\n","# Pre-compute embeddings for all original answers\n","print(\"Computing embeddings for original vocabulary...\")\n","original_embeddings = semantic_model.encode(original_answers, show_progress_bar=False)\n","\n","def find_semantically_similar_answer(new_answer, original_answers, original_embeddings):\n","    \"\"\"Find the most semantically similar answer in original_answers to new_answer\"\"\"\n","    # Get embedding for the new answer\n","    new_embedding = semantic_model.encode([new_answer], show_progress_bar=False)\n","\n","    # Calculate cosine similarity between new answer and all original answers\n","    similarities = cosine_similarity(new_embedding, original_embeddings)[0]\n","\n","    # Get the index of the most similar answer\n","    most_similar_idx = np.argmax(similarities)\n","    similarity_score = similarities[most_similar_idx]\n","\n","    return original_answers[most_similar_idx], similarity_score\n","\n","# Create a mapping dictionary for unseen answers\n","answer_mapping = {}\n","similarity_scores = {}\n","\n","print(\"Creating semantic mappings for unseen answers...\")\n","# Process all datasets to create mappings\n","for dataset_name, df in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n","    mapped_count = 0\n","    for ans in df['normalized_answer'].unique():\n","        if ans not in original_label2id and ans not in answer_mapping:\n","            similar_ans, score = find_semantically_similar_answer(ans, original_answers, original_embeddings)\n","            answer_mapping[ans] = similar_ans\n","            similarity_scores[ans] = score\n","            mapped_count += 1\n","\n","    print(f\"Dataset {dataset_name}: Mapped {mapped_count} unseen answers to semantically similar existing answers\")\n","\n","# Apply mapping to create mapped_answer column\n","def map_to_similar_answer(answer):\n","    if answer in original_label2id:\n","        return answer  # Already in the vocabulary\n","    return answer_mapping.get(answer, answer)  # Map to similar answer if needed\n","\n","train_df['mapped_answer'] = train_df['normalized_answer'].apply(map_to_similar_answer)\n","val_df['mapped_answer'] = val_df['normalized_answer'].apply(map_to_similar_answer)\n","test_df['mapped_answer'] = test_df['normalized_answer'].apply(map_to_similar_answer)\n","\n","# Print some statistics about the mapping\n","print(\"\\nAnswer mapping examples (with similarity scores):\")\n","if answer_mapping:\n","    # Sort by similarity score for better examples display\n","    sorted_mappings = sorted([(k, v, similarity_scores[k]) for k, v in answer_mapping.items()],\n","                            key=lambda x: x[2], reverse=True)\n","\n","    for i, (new_ans, similar_ans, score) in enumerate(sorted_mappings[:10]):  # Show first 10 examples\n","        print(f\"  '{new_ans}' -> '{similar_ans}' (similarity: {score:.3f})\")\n","\n","    if len(answer_mapping) > 10:\n","        print(f\"  ... and {len(answer_mapping) - 10} more mappings\")\n","else:\n","    print(\"  No mappings were created (all answers already in vocabulary)\")\n","\n","# --- Final Check ---\n","print(\"\\nFinal sizes:\")\n","print(f\"Train: {len(train_df)}\")\n","print(f\"Val: {len(val_df)}\")\n","print(f\"Test: {len(test_df)}\")\n","print(f\"Original vocabulary size: {len(original_label2id)}\")\n","print(f\"Total answer mappings created: {len(answer_mapping)}\")"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1747740160391,"user":{"displayName":"Anwesh Nayak","userId":"10812060260948452341"},"user_tz":-330},"id":"FUkT0UCHwsig"},"outputs":[],"source":["\"\"\"Create a custom dataset class\"\"\"\n","\n","class QnADataset(Dataset):\n","    def __init__(self, dataframe, image_dir, processor, label2id): # Processor is not strictly needed here anymore, but label2id is\n","        self.dataframe = dataframe\n","        self.image_dir = image_dir\n","        # self.processor = processor # Not used directly in __getitem__ anymore\n","        self.label2id = label2id\n","        self.text_to_num = self.generate_text_to_num_mapping()\n","\n","    def generate_text_to_num_mapping(self):\n","        # (Your existing generate_text_to_num_mapping method - keep as is)\n","        text_to_num = {\n","            \"zero\": \"0\", \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\",\n","            \"five\": \"5\", \"six\": \"6\", \"seven\": \"7\", \"eight\": \"8\", \"nine\": \"9\",\n","            \"ten\": \"10\", \"eleven\": \"11\", \"twelve\": \"12\", \"thirteen\": \"13\",\n","            \"fourteen\": \"14\", \"fifteen\": \"15\", \"sixteen\": \"16\", \"seventeen\": \"17\",\n","            \"eighteen\": \"18\", \"nineteen\": \"19\", \"twenty\": \"20\",\n","        }\n","        for i in range(21, 1001):\n","            text_to_num[str(i)] = str(i)\n","        return text_to_num\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        row = self.dataframe.iloc[idx]\n","        image_path = row['Image_Path']\n","        question_text = row[\"Question\"]  # Keep as raw text\n","\n","        # Use mapped_answer instead of Answer\n","        mapped_answer = row[\"mapped_answer\"].strip().lower()\n","\n","        # Convert text-based numbers to numerical strings if needed\n","        if mapped_answer in self.text_to_num:\n","            processed_answer_str = self.text_to_num[mapped_answer]\n","        else:\n","            processed_answer_str = mapped_answer\n","\n","        # Load PIL image\n","        try:\n","            pil_image = Image.open(image_path).convert(\"RGB\")\n","        except FileNotFoundError:\n","            print(f\"Error: Image not found at {image_path}\")\n","            # Handle appropriately: skip, return None, or use a placeholder\n","            # For now, let's re-raise to make it obvious during debugging\n","            raise\n","        except Exception as e:\n","            print(f\"Error loading image {image_path}: {e}\")\n","            raise\n","\n","        # Encode the answer string to an ID\n","        if processed_answer_str in self.label2id:\n","            answer_id = self.label2id[processed_answer_str]\n","        else:\n","            # This should be less common now since we're using mapped answers\n","            print(f\"Warning: Mapped answer '{processed_answer_str}' not found in label2id mapping. Item index: {idx}, Image: {row['Image_Path']}\")\n","            # We'll still include error handling for robustness\n","            raise ValueError(f\"Mapped answer '{processed_answer_str}' (from original '{row.get('Answer', 'N/A')}') not found in label2id mapping for image {row['Image_Path']}.\")\n","\n","        return {\n","            \"image\": pil_image,          # Return the PIL Image object\n","            \"question\": question_text,   # Return the raw question string\n","            \"labels\": torch.tensor(answer_id, dtype=torch.long) # Return the label as a tensor\n","        }"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1747740160419,"user":{"displayName":"Anwesh Nayak","userId":"10812060260948452341"},"user_tz":-330},"id":"MOlUexJFwsih"},"outputs":[],"source":["\"\"\" Prepare dataloaders \"\"\"\n","from functools import partial\n","\n","# Use original_label2id instead of extended_label2id\n","num_labels = len(original_label2id)\n","\n","# Directory containing the images\n","image_dir = \"/kaggle/input/filtered-small-amazon-qna\"\n","\n","# Create datasets with ORIGINAL labels and dataframes containing mapped_answer column\n","train_dataset = QnADataset(train_df, image_dir, processor, original_label2id)\n","val_dataset = QnADataset(val_df, image_dir, processor, original_label2id)  # Use full val_df, not filtered\n","test_dataset = QnADataset(test_df, image_dir, processor, original_label2id)  # Use full test_df, not filtered\n","\n","# Collate function with original num_labels\n","def collate_fn(batch, processor, num_classes=num_labels):\n","    \"\"\"ViLT-compatible collate function with one-hot encoding\"\"\"\n","    # Filter out invalid entries\n","    valid_batch = [\n","        item for item in batch\n","        if item is not None\n","        and isinstance(item.get(\"image\"), Image.Image)\n","        and item.get(\"question\")\n","        and item.get(\"labels\") is not None\n","    ]\n","\n","    if not valid_batch:\n","        return None\n","\n","    # Process valid items\n","    images = [item[\"image\"] for item in valid_batch]\n","    texts = [item[\"question\"] for item in valid_batch]\n","    labels = [item[\"labels\"] for item in valid_batch]  # Should be class indices\n","\n","    # Process through processor\n","    try:\n","        encoding = processor(\n","            images=images,\n","            text=texts,\n","            return_tensors=\"pt\",\n","            padding=\"longest\",\n","            truncation=True,\n","            max_length=512\n","        )\n","    except Exception as e:\n","        print(f\"Skipping batch: {str(e)}\")\n","        return None\n","\n","    # Convert labels to one-hot encoding\n","    batch_size = len(labels)\n","    one_hot_labels = torch.zeros(batch_size, num_classes)\n","    for i, label in enumerate(labels):\n","        one_hot_labels[i, label] = 1.0\n","\n","    encoding[\"labels\"] = one_hot_labels\n","    return encoding\n","\n","# Create DataLoaders with proper partial binding\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=16,\n","    shuffle=True,\n","    collate_fn=partial(collate_fn, processor=processor),  # Keyword argument binding\n","    drop_last=True\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=16,\n","    shuffle=False,\n","    collate_fn=partial(collate_fn, processor=processor)  # Keyword argument binding\n",")\n","\n","test_loader = DataLoader(\n","    test_dataset,\n","    batch_size=16,\n","    shuffle=False,\n","    collate_fn=partial(collate_fn, processor=processor)  # Keyword argument binding\n",")"]},{"cell_type":"markdown","metadata":{"id":"2HZyxnhYwsih"},"source":["# Fine tuning part"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1747740160425,"user":{"displayName":"Anwesh Nayak","userId":"10812060260948452341"},"user_tz":-330},"id":"9qpAPaM2wsii"},"outputs":[],"source":["import os\n","import time\n","import torch\n","from tqdm import tqdm\n","from transformers import get_linear_schedule_with_warmup\n","import torch.optim as optim\n","from peft import LoraConfig, get_peft_model\n","\n","# --- Config ---\n","NUM_EPOCHS    = 20\n","LEARNING_RATE = 1e-4\n","WEIGHT_DECAY  = 1e-2\n","WARMUP_RATIO  = 0.1       # 10% of total steps\n","MAX_GRAD_NORM = 1.0\n","OUTPUT_DIR    = \"/content/drive/MyDrive/VR Project 2/Fine Tune/models/best\"\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# --- Model + LoRA setup (UPDATED) ---\n","original_model = ViltForQuestionAnswering.from_pretrained(\n","    \"dandelin/vilt-b32-finetuned-vqa\",\n","    # Using original vocabulary from the pretrained model\n","    num_labels=len(original_label2id),\n","    id2label=model_config_source.config.id2label,\n","    label2id=original_label2id\n","    # Removed ignore_mismatched_sizes since we're using original sizes\n",")\n","\n","lora_config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    target_modules=[\"query\", \"value\"],\n","    lora_dropout=0.1,\n","    bias=\"none\",\n","    modules_to_save=[\"classifier\"]\n",")\n","model = get_peft_model(original_model, lora_config)\n","model.to(device)\n","model.print_trainable_parameters()\n","\n","# --- Optimizer + Scheduler + AMP Scaler ---\n","optimizer = optim.AdamW(\n","    model.parameters(),\n","    lr=LEARNING_RATE,\n","    weight_decay=WEIGHT_DECAY\n",")\n","\n","total_steps   = len(train_loader) * NUM_EPOCHS\n","warmup_steps  = int(WARMUP_RATIO * total_steps)\n","scheduler     = get_linear_schedule_with_warmup(\n","    optimizer, warmup_steps, total_steps\n",")\n","\n","scaler = torch.cuda.amp.GradScaler()\n","\n","best_val_loss = float('inf')\n","patience, patience_counter = 10, 0\n","\n","for epoch in range(1, NUM_EPOCHS + 1):\n","# for epoch in range(1):\n","    print(f\"\\n-- Epoch {epoch}/{NUM_EPOCHS} --\")\n","    t0_epoch = time.time()\n","\n","    # ---- TRAIN ----\n","    model.train()\n","    train_loss = 0.0\n","    train_batches = 0\n","    pbar = tqdm(train_loader, desc=\"Train\", leave=False)\n","    for batch in pbar:\n","        # Skip None batches\n","        if batch is None:\n","            continue\n","\n","        batch = {k: v.to(device) for k,v in batch.items() if v is not None}\n","\n","        optimizer.zero_grad()\n","        with torch.amp.autocast('cuda'):  # Updated to new format\n","            outputs = model(**batch)\n","            loss = outputs.loss\n","\n","        scaler.scale(loss).backward()\n","        # clip grads\n","        scaler.unscale_(optimizer)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n","        scaler.step(optimizer)\n","        scaler.update()\n","        scheduler.step()\n","\n","        train_loss += loss.item()\n","        train_batches += 1\n","        pbar.set_postfix(loss=loss.item())\n","\n","    avg_train = train_loss / max(train_batches, 1)  # Avoid division by zero\n","\n","    # ---- VALIDATION ----\n","    model.eval()\n","    val_loss = 0.0\n","    val_batches = 0\n","    with torch.no_grad():\n","        pbar = tqdm(val_loader, desc=\"Valid\", leave=False)\n","        for batch in pbar:\n","            # Skip None batches\n","            if batch is None:\n","                continue\n","\n","            batch = {k: v.to(device) for k,v in batch.items() if v is not None}\n","            with torch.amp.autocast('cuda'):  # Updated to new format\n","                loss = model(**batch).loss\n","            val_loss += loss.item()\n","            val_batches += 1\n","            pbar.set_postfix(loss=loss.item())\n","\n","    avg_val = val_loss / max(val_batches, 1)  # Avoid division by zero\n","    print(f\"Train Loss: {avg_train:.4f} | Val Loss: {avg_val:.4f} | Time: {(time.time()-t0_epoch):.1f}s\")\n","\n","    # ---- Early Stopping & Checkpointing ----\n","    if avg_val < best_val_loss:\n","        best_val_loss = avg_val\n","        patience_counter = 0\n","        print(f\" New best! Saving to {OUTPUT_DIR}\")\n","        model.save_pretrained(OUTPUT_DIR)\n","    else:\n","        patience_counter += 1\n","        if patience_counter >= patience:\n","            print(f\"Stopping early (no improvement for {patience} epochs).\")\n","            break\n","\n","print(\"\\n=== Training Complete ===\")\n","print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n","print(f\"Best model saved at: {OUTPUT_DIR}\")"]},{"cell_type":"markdown","metadata":{"id":"cldkURBewsij"},"source":["# Metrics"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2674,"status":"ok","timestamp":1747740163102,"user":{"displayName":"Anwesh Nayak","userId":"10812060260948452341"},"user_tz":-330},"id":"H6MmwpeMwsij","outputId":"92e2905e-bec7-4f69-873a-2c6e7df7a9d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bert-score in /usr/local/lib/python3.11/dist-packages (0.3.13)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.6.0+cu124)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.31.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.4.26)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n","fatal: destination path 'BARTScore' already exists and is not an empty directory.\n"]}],"source":["!pip install bert-score\n","!git clone https://github.com/neulab/BARTScore.git\n","import sys\n","sys.path.append(\"./BARTScore\")\n","# Now import\n","from bart_score import BARTScorer"]},{"cell_type":"markdown","source":["# Baseline"],"metadata":{"id":"zV8IXtuffjUC"}},{"cell_type":"code","source":["import sys\n","import time\n","import torch\n","from tqdm import tqdm\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","\n","# 1. Add local BARTScore code into Python’s import path\n","sys.path.append(\"./BARTScore\")\n","\n","# 2. Semantic‐similarity imports\n","from bert_score import score as bert_score\n","from bart_score import BARTScorer\n","\n","# 3. PEFT & model imports\n","from transformers import ViltForQuestionAnswering\n","from peft import PeftModel\n","\n","# 4. Device setup\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# 5. Reload base + LoRA‐finetuned model\n","#    (assumes you previously saved to OUTPUT_DIR)\n","OUTPUT_DIR = \"/content/drive/MyDrive/VR Project 2/Training and Evaluation Scripts/Fine Tune/models/VILT_best\"\n","model = ViltForQuestionAnswering.from_pretrained(\n","    \"dandelin/vilt-b32-finetuned-vqa\",\n","    num_labels=len(original_label2id),\n","    id2label=model_config_source.config.id2label,\n","    label2id=original_label2id,\n","    ignore_mismatched_sizes=True\n",")\n","# model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n","model.to(device)\n","model.eval()\n","\n","# 6. Accumulators\n","all_pred_ids   = []\n","all_true_ids   = []\n","all_pred_texts = []\n","all_true_texts = []\n","\n","# --- Start overall timer ---\n","t0_overall = time.time()\n","\n","# 7. Inference + gather labels/texts with progress bar\n","t0_loop = time.time()\n","for batch in tqdm(test_loader, desc=\"Evaluating batches\"):\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    outputs = model(**batch)\n","    logits  = outputs.logits\n","\n","    # Predicted & true IDs\n","    pred_ids = logits.argmax(dim=-1)\n","    true_ids = batch[\"labels\"].argmax(dim=-1)\n","\n","    # Flatten for metrics\n","    pred_flat = pred_ids.view(-1).cpu().numpy()\n","    true_flat = true_ids.view(-1).cpu().numpy()\n","    all_pred_ids.extend(pred_flat)\n","    all_true_ids.extend(true_flat)\n","\n","    # Convert to label strings\n","    all_pred_texts.extend([model.config.id2label[i] for i in pred_flat])\n","    all_true_texts.extend([model.config.id2label[i] for i in true_flat])\n","t1_loop = time.time()\n","print(f\"\\nInference & gathering took {t1_loop - t0_loop:.2f}s\")\n","\n","# 8. Classification metrics\n","t0_cls = time.time()\n","accuracy  = accuracy_score(all_true_ids, all_pred_ids)\n","precision = precision_score(all_true_ids, all_pred_ids, average=\"macro\", zero_division=0)\n","recall    = recall_score(all_true_ids, all_pred_ids, average=\"macro\", zero_division=0)\n","f1        = f1_score(all_true_ids, all_pred_ids, average=\"macro\", zero_division=0)\n","t1_cls = time.time()\n","\n","print(f\"\\nClassification metrics computed in {t1_cls - t0_cls:.2f}s\")\n","print(\"=== Classification Metrics ===\")\n","print(f\"Accuracy      : {accuracy:.4f}\")\n","print(f\"Precision (M) : {precision:.4f}\")\n","print(f\"Recall    (M) : {recall:.4f}\")\n","print(f\"F1 Score  (M) : {f1:.4f}\")\n","\n","# 9. BERTScore (semantic similarity)\n","t0_bert = time.time()\n","bert_p, bert_r, bert_f1 = bert_score(\n","    all_pred_texts,\n","    all_true_texts,\n","    lang=\"en\",\n","    model_type=\"bert-base-uncased\",\n","    rescale_with_baseline=True\n",")\n","t1_bert = time.time()\n","print(f\"\\nBERTScore computed in {t1_bert - t0_bert:.2f}s\")\n","print(\"=== BERTScore ===\")\n","print(f\"Precision : {bert_p.mean().item():.4f}\")\n","print(f\"Recall    : {bert_r.mean().item():.4f}\")\n","print(f\"F1        : {bert_f1.mean().item():.4f}\")\n","\n","# 10. BARTScore (semantic entailment)\n","t0_bart = time.time()\n","bart_scorer = BARTScorer(device=device.type, checkpoint=\"facebook/bart-large-cnn\")\n","bart_scores = bart_scorer.score(\n","    all_pred_texts,\n","    all_true_texts,\n","    batch_size=8\n",")\n","t1_bart = time.time()\n","mean_bart = sum(bart_scores) / len(bart_scores)\n","print(f\"\\nBARTScore computed in {t1_bart - t0_bart:.2f}s\")\n","print(\"=== BARTScore ===\")\n","print(f\"Mean score: {mean_bart:.4f}\")\n","\n","# --- End overall timer ---\n","t1_overall = time.time()\n","print(f\"\\nTotal evaluation time: {t1_overall - t0_overall:.2f}s\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yxCgj5nBVjvd","executionInfo":{"status":"ok","timestamp":1747740624886,"user_tz":-330,"elapsed":51909,"user":{"displayName":"Anwesh Nayak","userId":"10812060260948452341"}},"outputId":"12588b72-7dab-4994-e67f-d008f1795db0"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating batches: 100%|██████████| 147/147 [00:41<00:00,  3.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Inference & gathering took 41.96s\n","\n","Classification metrics computed in 0.02s\n","=== Classification Metrics ===\n","Accuracy      : 0.2777\n","Precision (M) : 0.0510\n","Recall    (M) : 0.0585\n","F1 Score  (M) : 0.0452\n","\n","BERTScore computed in 0.93s\n","=== BERTScore ===\n","Precision : 0.6376\n","Recall    : 0.6286\n","F1        : 0.6314\n","\n","BARTScore computed in 8.28s\n","=== BARTScore ===\n","Mean score: -5.4490\n","\n","Total evaluation time: 51.19s\n"]}]},{"cell_type":"markdown","source":["# Finetune"],"metadata":{"id":"EaZ-bArFfmiS"}},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uu1RlB6Bwsij","outputId":"468939a9-4892-466a-ca6d-d796462cfdca","executionInfo":{"status":"ok","timestamp":1747740531800,"user_tz":-330,"elapsed":52014,"user":{"displayName":"Anwesh Nayak","userId":"10812060260948452341"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating batches: 100%|██████████| 147/147 [00:42<00:00,  3.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Inference & gathering took 42.13s\n","\n","Classification metrics computed in 0.02s\n","=== Classification Metrics ===\n","Accuracy      : 0.6231\n","Precision (M) : 0.3336\n","Recall    (M) : 0.3432\n","F1 Score  (M) : 0.3159\n","\n","BERTScore computed in 0.87s\n","=== BERTScore ===\n","Precision : 0.8163\n","Recall    : 0.8141\n","F1        : 0.8143\n","\n","BARTScore computed in 8.30s\n","=== BARTScore ===\n","Mean score: -3.8496\n","\n","Total evaluation time: 51.32s\n"]}],"source":["import sys\n","import time\n","import torch\n","from tqdm import tqdm\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","\n","# 1. Add local BARTScore code into Python’s import path\n","sys.path.append(\"./BARTScore\")\n","\n","# 2. Semantic‐similarity imports\n","from bert_score import score as bert_score\n","from bart_score import BARTScorer\n","\n","# 3. PEFT & model imports\n","from transformers import ViltForQuestionAnswering\n","from peft import PeftModel\n","\n","# 4. Device setup\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# 5. Reload base + LoRA‐finetuned model\n","#    (assumes you previously saved to OUTPUT_DIR)\n","OUTPUT_DIR = \"/content/drive/MyDrive/VR Project 2/Training and Evaluation Scripts/Fine Tune/models/VILT_best\"\n","# base_model = ViltForQuestionAnswering.from_pretrained(\n","#     \"dandelin/vilt-b32-finetuned-vqa\",\n","#     num_labels=len(original_label2id),\n","#     id2label=model_config_source.config.id2label,\n","#     label2id=original_label2id,\n","#     ignore_mismatched_sizes=True\n","# )\n","\n","model = ViltForQuestionAnswering.from_pretrained(OUTPUT_DIR)\n","model.to(device)\n","model.eval()\n","\n","# 6. Accumulators\n","all_pred_ids   = []\n","all_true_ids   = []\n","all_pred_texts = []\n","all_true_texts = []\n","\n","# --- Start overall timer ---\n","t0_overall = time.time()\n","\n","# 7. Inference + gather labels/texts with progress bar\n","t0_loop = time.time()\n","for batch in tqdm(test_loader, desc=\"Evaluating batches\"):\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    outputs = model(**batch)\n","    logits  = outputs.logits\n","\n","    # Predicted & true IDs\n","    pred_ids = logits.argmax(dim=-1)\n","    true_ids = batch[\"labels\"].argmax(dim=-1)\n","\n","    # Flatten for metrics\n","    pred_flat = pred_ids.view(-1).cpu().numpy()\n","    true_flat = true_ids.view(-1).cpu().numpy()\n","    all_pred_ids.extend(pred_flat)\n","    all_true_ids.extend(true_flat)\n","\n","    # Convert to label strings\n","    all_pred_texts.extend([model.config.id2label[i] for i in pred_flat])\n","    all_true_texts.extend([model.config.id2label[i] for i in true_flat])\n","t1_loop = time.time()\n","print(f\"\\nInference & gathering took {t1_loop - t0_loop:.2f}s\")\n","\n","# 8. Classification metrics\n","t0_cls = time.time()\n","accuracy  = accuracy_score(all_true_ids, all_pred_ids)\n","precision = precision_score(all_true_ids, all_pred_ids, average=\"macro\", zero_division=0)\n","recall    = recall_score(all_true_ids, all_pred_ids, average=\"macro\", zero_division=0)\n","f1        = f1_score(all_true_ids, all_pred_ids, average=\"macro\", zero_division=0)\n","t1_cls = time.time()\n","\n","print(f\"\\nClassification metrics computed in {t1_cls - t0_cls:.2f}s\")\n","print(\"=== Classification Metrics ===\")\n","print(f\"Accuracy      : {accuracy:.4f}\")\n","print(f\"Precision (M) : {precision:.4f}\")\n","print(f\"Recall    (M) : {recall:.4f}\")\n","print(f\"F1 Score  (M) : {f1:.4f}\")\n","\n","# 9. BERTScore (semantic similarity)\n","t0_bert = time.time()\n","bert_p, bert_r, bert_f1 = bert_score(\n","    all_pred_texts,\n","    all_true_texts,\n","    lang=\"en\",\n","    model_type=\"bert-base-uncased\",\n","    rescale_with_baseline=True\n",")\n","t1_bert = time.time()\n","print(f\"\\nBERTScore computed in {t1_bert - t0_bert:.2f}s\")\n","print(\"=== BERTScore ===\")\n","print(f\"Precision : {bert_p.mean().item():.4f}\")\n","print(f\"Recall    : {bert_r.mean().item():.4f}\")\n","print(f\"F1        : {bert_f1.mean().item():.4f}\")\n","\n","# 10. BARTScore (semantic entailment)\n","t0_bart = time.time()\n","bart_scorer = BARTScorer(device=device.type, checkpoint=\"facebook/bart-large-cnn\")\n","bart_scores = bart_scorer.score(\n","    all_pred_texts,\n","    all_true_texts,\n","    batch_size=8\n",")\n","t1_bart = time.time()\n","mean_bart = sum(bart_scores) / len(bart_scores)\n","print(f\"\\nBARTScore computed in {t1_bart - t0_bart:.2f}s\")\n","print(\"=== BARTScore ===\")\n","print(f\"Mean score: {mean_bart:.4f}\")\n","\n","# --- End overall timer ---\n","t1_overall = time.time()\n","print(f\"\\nTotal evaluation time: {t1_overall - t0_overall:.2f}s\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":7375098,"sourceId":11748083,"sourceType":"datasetVersion"},{"datasetId":7375132,"sourceId":11748127,"sourceType":"datasetVersion"}],"dockerImageVersionId":31040,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat":4,"nbformat_minor":0}